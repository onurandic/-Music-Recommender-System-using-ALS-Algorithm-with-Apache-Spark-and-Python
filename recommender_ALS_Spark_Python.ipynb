{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!conda install -c cyclus java-jdk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "from pyspark.mllib.recommendation import *\n",
    "import random\n",
    "from operator import *\n",
    "from collections import defaultdict\n",
    "from pyspark import SparkContext, SparkConf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Spark Context\n",
    "spark = SparkContext.getOrCreate()\n",
    "spark.stop()\n",
    "spark = SparkContext('local','Recommender')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import test files from location into RDD variables\n",
    "artistData = spark.textFile('./data_raw/artist_data.txt').map(lambda s:(int(s.split(\"\\t\")[0]),s.split(\"\\t\")[1]))\n",
    "artistAlias = spark.textFile('./data_raw/artist_alias.txt')\n",
    "userArtistData = spark.textFile('./data_raw/user_artist_data.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split a sequence into seperate entities and store as int\n",
    "\n",
    "userArtistData = userArtistData.map(lambda s:(int(s.split(\" \")[0]),int(s.split(\" \")[1]),int(s.split(\" \")[2])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an RDD consisting of 'userid' and 'playcount' objects of original tuple\n",
    "userSum = userArtistData.map(lambda x:(x[0],x[2]))\n",
    "playCount1 = userSum.map(lambda x: (x[0],x[1])).reduceByKey(lambda a,b : a+b)\n",
    "playCount2 = userSum.map(lambda x: (x[0],1)).reduceByKey(lambda a,b:a+b)\n",
    "playSumAndCount = playCount1.leftOuterJoin(playCount2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dictionary of the 'artistAlias' dataset\n",
    "\n",
    "artistAliasDictionary = {}\n",
    "dataValue = artistAlias.map(lambda s:(int(s.split(\"\\t\")[0]),int(s.split(\"\\t\")[1])))\n",
    "for temp in dataValue.collect():\n",
    "    artistAliasDictionary[temp[0]] = temp[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If artistid exists, replace with artistsid from artistAlias, else retain original\n",
    "\n",
    "userArtistData = userArtistData.map(lambda x: (x[0], artistAliasDictionary[x[1]] if x[1] in artistAliasDictionary else x[1], x[2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count instances by key and store in broadcast variable\n",
    "\n",
    "playSumAndCount = playSumAndCount.map(lambda x: (x[0],x[1][0],int(x[1][0]/x[1][1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute and display users with the highest playcount along with their mean playcount across artists\n",
    "\n",
    "TopThree = playSumAndCount.top(3,key=lambda x: x[1])\n",
    "for i in TopThree:\n",
    "    print('User '+str(i[0])+' has a total play count of '+str(i[1])+' and a mean play count of '+str(i[2])+'.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the 'userArtistData' dataset into training, validation and test datasets. Store in cache for frequent access\n",
    "\n",
    "trainData, validationData, testData = userArtistData.randomSplit((0.4,0.4,0.2),seed=13)\n",
    "trainData.cache()\n",
    "validationData.cache()\n",
    "testData.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def modelEval(model, dataset):\n",
    "    \n",
    "    # All artists in the 'userArtistData' dataset\n",
    "    AllArtists = spark.parallelize(set(userArtistData.map(lambda x:x[1]).collect()))\n",
    "    \n",
    "    # Set of all users in the current (Validation/Testing) dataset\n",
    "    AllUsers = spark.parallelize(set(dataset.map(lambda x:x[0]).collect()))\n",
    "    \n",
    "    # Create a dictionary of (key, values) for current (Validation/Testing) dataset\n",
    "    ValidationAndTestingDictionary ={}\n",
    "    for temp in AllUsers.collect():\n",
    "        tempFilter = dataset.filter(lambda x:x[0] == temp).collect()\n",
    "        for item in tempFilter:\n",
    "            if temp in ValidationAndTestingDictionary:\n",
    "                ValidationAndTestingDictionary[temp].append(item[1])\n",
    "            else:\n",
    "                ValidationAndTestingDictionary[temp] = [item[1]]           \n",
    "    \n",
    "    # Create a dictionary of (key, values) for training dataset\n",
    "    TrainingDictionary = {}\n",
    "    for temp in AllUsers.collect():\n",
    "        tempFilter = trainData.filter(lambda x:x[0] == temp).collect()\n",
    "        for item in tempFilter:\n",
    "            if temp in TrainingDictionary:\n",
    "                TrainingDictionary[temp].append(item[1])\n",
    "            else:\n",
    "                TrainingDictionary[temp] = [item[1]]\n",
    "        \n",
    "    # For each user, calculate the prediction score i.e. similarity between predicted and actual artists\n",
    "    PredictionScore = 0.00\n",
    "    for temp in AllUsers.collect():\n",
    "        ArtistPrediction =  AllArtists.map(lambda x:(temp,x))\n",
    "        ModelPrediction = model.predictAll(ArtistPrediction)\n",
    "        tempFilter = ModelPrediction.filter(lambda x :not x[1] in TrainingDictionary[x[0]])\n",
    "        topPredictions = tempFilter.top(len(ValidationAndTestingDictionary[temp]),key=lambda x:x[2])\n",
    "        l=[]\n",
    "        for i in topPredictions:\n",
    "            l.append(i[1])\n",
    "        PredictionScore+=len(set(l).intersection(ValidationAndTestingDictionary[temp]))/len(ValidationAndTestingDictionary[temp])    \n",
    "\n",
    "    # Print average score of the model for all users for the specified rank\n",
    "    print(\"The model score for rank \"+str(model.rank)+\" is ~\"+str(PredictionScore/len(ValidationAndTestingDictionary)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    " rankList = [10,50,200]\n",
    " for rank in rankList:\n",
    "     model = ALS.trainImplicit(trainData, rank , seed=5028)\n",
    "     modelEval(model,validationData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bestModel = ALS.trainImplicit(trainData, rank=10, seed=5028)\n",
    "modelEval(bestModel, testData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the top 5 artists for a particular user and list their names\n",
    "TopFive = bestModel.recommendProducts(1059637,5)\n",
    "for item in range(0,5):\n",
    "    print(\"Artist \"+str(item)+\": \"+artistData.filter(lambda x:x[0] == TopFive[item][1]).collect()[0][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
